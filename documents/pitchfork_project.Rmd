---
title: "Examining trends in popular music criticism using the 'Pitchfork database'"
author: "Tim Metcalfe"
date: "3 May 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Reviewers exert great influence on the consumption of contemporary popular music. For example, the 'Pitchfork effect' -- named after the [hugely popular online music magazine](http://pitchfork.com) -- refers to the tendency for sales to increase immediately after acclaim by a well-known reviewer. Therefore, it is worthwhile to understand, in-depth, any potential trends and biases in critical opinion. Recently, an [SQL database was compiled](https://github.com/nsgrantham/pitchfork-reviews), containing over 17,000 music reviews published by Pitchfork between 1999 and 2016. This dataset offered a unique opportunity to study differences in critical opinion, both as a function of time and of different individual reviewers. However, no information about musical genre was included, which could be highly informative with respect to highlighting more overarching trends and/or biases in reviews. To this end, I began updating the 'Pitchfork database' by adding genre labels for each review, to allow for some additional analyses.

## Updating the Pitchfork database

### Step 1
Initially, (and somewhat naively), I built a web scraper with `Python`, to search for each album's genre online and download the result. While this did help provide a 'first guess' at the genre, the approach was fraught with errors, and lots of albums were too obscure to be found using this method.

### Step 2
Next, I manually (and somewhat painstakingly) searched through the actual reviews for clues. In some cases, Pitchfork are very explicit about the genre being reviwed, but in other cases not so much. For the latter, I searched other review sites, to find the best possible consensus. I coded each genre at the most specific level possible, initially (*e.g.* if Pitchfork described something as 'deep house', it got labeleled as such, as opposed to somethig more generic like 'dance'. This approach resulted in 173 genres!

### Step 3
Lastly, any genres that accounted for < 1% of the sample were either:

A) reclassified at a higher level (*e.g.* death metal, black metal *etc.* become 'extreme metal'), or 
B) excluded from the dataset (e.g. spoken word, comedy, classical).

The reason for this approach was, in a sense, to assume nothing about the genres to begin with -- to let the data decide which genres the reviews should be split into.

## Genre counts

Once everything had been tagged, I was left with 39 genres:

![center](C:\Users\megaw_000\Documents\GitHub\megawalrus.github.io\figs\genre_counts.png)

## Genre mean scores

Here's a plot of mean scores by genre:

![center](C:\Users\megaw_000\Documents\GitHub\megawalrus.github.io\figs\genre_mean_scores.png)
